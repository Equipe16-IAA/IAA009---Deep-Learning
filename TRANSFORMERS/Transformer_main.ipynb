{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPVu8UGZ46pjOdAsfevZGhD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Equipe16-IAA/IAA009---Deep-Learning/blob/main/TRANSFORMERS/Transformer_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importação da biblioteca\n"
      ],
      "metadata": {
        "id": "XF7ZiO6UY35N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyQj6ednY03C",
        "outputId": "cab95e03-fc80-41aa-cd7c-1db4c6076832"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.15.0\n",
            "Uninstalling tensorflow-2.15.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/import_pb_to_tensorboard\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.10/dist-packages/tensorflow-2.15.0.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/tensorflow/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled tensorflow-2.15.0\n",
            "Collecting tensorflow==2.15.0\n",
            "  Using cached tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n",
            "Using cached tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "Installing collected packages: tensorflow\n",
            "Successfully installed tensorflow-2.15.0\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: tensorflow-text==2.15.0 in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text==2.15.0) (0.16.1)\n",
            "Requirement already satisfied: tensorflow<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub>=0.13.0->tensorflow-text==2.15.0) (2.15.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==2.15.0\n",
        "!pip install tensorflow datasets\n",
        "!pip install -U tensorflow-text==2.15.0\n",
        "\n",
        "import collections\n",
        "import logging\n",
        "import os\n",
        "import pathlib\n",
        "import string\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as text\n",
        "\n",
        "logging.getLogger('tensorflow').setLevel(logging.ERROR)  # suppress warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carrega a base de dados"
      ],
      "metadata": {
        "id": "z_6Rxb_WZXsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
        "                               as_supervised=True)\n",
        "train_examples, val_examples = examples['train'], examples['validation']"
      ],
      "metadata": {
        "id": "ux0LzZH3ZcX4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verifica o dataset"
      ],
      "metadata": {
        "id": "n88MFUpPZnMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifica o dataset\n",
        "for pt_examples, en_examples in train_examples.batch(3).take(1):\n",
        "  for pt in pt_examples.numpy():\n",
        "    print(pt.decode('utf-8'))\n",
        "\n",
        "  print()\n",
        "\n",
        "  for en in en_examples.numpy():\n",
        "    print(en.decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEWjjap7Znjp",
        "outputId": "70881459-7b33-4ea4-82a4-5f6c3b1efa45"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\n",
            "mas e se estes fatores fossem ativos ?\n",
            "mas eles não tinham a curiosidade de me testar .\n",
            "\n",
            "and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
            "but what if it were active ?\n",
            "but they did n't test for curiosity .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenização e Destokenização"
      ],
      "metadata": {
        "id": "a14FW5fAZ6c8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenização e Destokenização do texto\n",
        "\n",
        "model_name = \"ted_hrlr_translate_pt_en_converter\"\n",
        "\n",
        "tf.keras.utils.get_file(f\"{model_name}.zip\", f\"https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip\", cache_dir='.', cache_subdir='', extract=True)\n",
        "\n",
        "# Tem 2 tokenizers: um pt outr em en\n",
        "# tokenizers .en tokeniza e destokeniza\n",
        "tokenizers = tf.saved_model.load(model_name)"
      ],
      "metadata": {
        "id": "ctuDG9l1Z892"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline de Entrada"
      ],
      "metadata": {
        "id": "DaUMgcvvalAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline de entrada\n",
        "# Codificar/tokenizar lotes de texto puro\n",
        "\n",
        "def tokenize_pairs(pt, en):\n",
        "    pt = tokenizers.pt.tokenize(pt)\n",
        "    # Converte ragged (irregular, tam variável) para dense\n",
        "    # Faz padding com zeros.\n",
        "    pt = pt.to_tensor()\n",
        "\n",
        "    en = tokenizers.en.tokenize(en)\n",
        "    # ragged -> dense\n",
        "    en = en.to_tensor()\n",
        "\n",
        "    return pt, en"
      ],
      "metadata": {
        "id": "OcFB3Mryalh0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline simples: processa, embaralha, agrupa os dados, prefetch\n",
        "# Datasets de entrada terminam com prefetch\n",
        "\n",
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "def make_batches(ds):\n",
        "  return (\n",
        "      ds\n",
        "      .cache()\n",
        "      .shuffle(BUFFER_SIZE)\n",
        "      .batch(BATCH_SIZE)\n",
        "      .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "      .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "train_batches = make_batches(train_examples)\n",
        "val_batches = make_batches(val_examples)\n"
      ],
      "metadata": {
        "id": "suROAOzebN9D"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Codificação posicional"
      ],
      "metadata": {
        "id": "uaIaDUzwbtni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Codificação posicional\n",
        "\n",
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # sin em índices pares no array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # cos em índices ímpares no array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  # newaxis, aumenta a dimensão [] -> [ [] ]\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "3RQd83Srbv81"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Codificação posicional\n",
        "n, d = 2048, 512\n",
        "pos_encoding = positional_encoding(n, d)\n",
        "print(pos_encoding.shape)\n",
        "pos_encoding = pos_encoding[0]\n",
        "\n",
        "# Arrumar as dimensões\n",
        "pos_encoding = tf.reshape(pos_encoding, (n, d//2, 2))\n",
        "pos_encoding = tf.transpose(pos_encoding, (2, 1, 0))\n",
        "pos_encoding = tf.reshape(pos_encoding, (d, n))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJWn6V1tclTt",
        "outputId": "54aa5577-b299-45a2-db27-9a3f1ab46e2b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 2048, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cria a Máscara de 0s e 1s"
      ],
      "metadata": {
        "id": "yaLhXXd9dHYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria uma mascara de 0 e 1, 0 para quando há valor e 1 quando não há\n",
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "  # Add extradimensions to add the padding\n",
        "  # to the attention logits\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "# Máscara futura, usada no decoder\n",
        "def create_look_ahead_mask(size):\n",
        "  # zera o triângulo inferior\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask  # (seq_len, seq_len)"
      ],
      "metadata": {
        "id": "WZJcHCLPdKsd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Função de atenção"
      ],
      "metadata": {
        "id": "sLn16zWLdvqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função de atenção\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  # Q K^T\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  #Converte matmul qk para float32\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  # divide por sqrt (d_k)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # Soma a máscara, e os valores faltantes serão um número próximo a -inf\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "  # Softmax normaliza os dados, somam 1. // (..., seq_len_q, seq_len_k)\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "  return output, attention_weights"
      ],
      "metadata": {
        "id": "qPcmlhYjdxha"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Atenção Multi-cabeças"
      ],
      "metadata": {
        "id": "EhRbZGKee5ZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Atenção Multi-cabeças\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    assert d_model % self.num_heads == 0\n",
        "\n",
        "    self.depth = d_model // self.num_heads\n",
        "\n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def split_heads(self, x, batch_size):\n",
        "    # Separa a última dimensão em (num_heads, depth). Transpões o resultado para o shape (batch_size, num_heads, self.depth)\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "\n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "\n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "    # Calcula a atenção para cada cabeça (de forma matricial)\n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "\n",
        "    # Troca a dimensão 2 com 1, para acertar o num_heads\n",
        "    # (batch_size, seq_len_q, num_heads, depth)\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "    # Concatena os vbalores em: (batch_size, seq_len_q, d_model)\n",
        "    concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    return output, attention_weights"
      ],
      "metadata": {
        "id": "S0k63Ou2e8az"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cria rede feed-foward pontual"
      ],
      "metadata": {
        "id": "if_GNrgd10E3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model) # (batch_size, seq_len, d_model)\n",
        "  ])"
      ],
      "metadata": {
        "id": "sw-EjVpR13pQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Camada do Codificador"
      ],
      "metadata": {
        "id": "19djVoLP2ObI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    return out2"
      ],
      "metadata": {
        "id": "FAVqw3102SHw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Camada do Decodificador"
      ],
      "metadata": {
        "id": "jHvUtPtd3QoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "\n",
        "  def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    # (batch_size, target_seq_len, d_model)\n",
        "    attn1, att_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "    # (batch_size, target_seq_len, d_model)\n",
        "    attn2, att_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    return out3, att_weights_block1, att_weights_block2"
      ],
      "metadata": {
        "id": "uO9urHlS3TVi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder completo"
      ],
      "metadata": {
        "id": "fzu_bG3b45ee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    # adding embedding and position encoding.\n",
        "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "    return x  # (batch_size, target_seq_len, d_model)"
      ],
      "metadata": {
        "id": "CqvW3DaU47d6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder completo"
      ],
      "metadata": {
        "id": "mPnbNF9m5yij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
        "                       for _ in range(num_layers)]\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "\n",
        "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
        "\n",
        "      attention_weights[f'decoder_layer{i+1}_block1'.format(i+1)] = block1\n",
        "      attention_weights[f'decoder_layer{i+1}_block2'.format(i+1)] = block2\n",
        "\n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\n",
        "    return x, attention_weights"
      ],
      "metadata": {
        "id": "3CvEq3dA50fV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Completo"
      ],
      "metadata": {
        "id": "PHzSRzHr7D0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
        "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, inputs, training):\n",
        "    # Keras models prefer if you pass all your inputs the first argument\n",
        "    inp, tar = inputs\n",
        "\n",
        "    enc_padding_mask, look_ahead_mask, dec_padding_mask = self.create_masks(inp, tar)\n",
        "\n",
        "    # (batch_size, target_seq_len, d_model)\n",
        "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "    dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "    # (batch_size, target_seq_len, target_vocab_size)\n",
        "    final_output = self.final_layer(dec_output)\n",
        "\n",
        "    return final_output, attention_weights\n",
        "\n",
        "\n",
        "  def create_masks(self, inp, tar):\n",
        "    # Encoder padding mask\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # Used in 2nd attention block in the decoder\n",
        "    # This padding mask is used to mask the endocer outputs\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # Used in the 1st attention block in the decoder\n",
        "    # It is used to pad and mask future tokens in the input received by the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "    return enc_padding_mask, look_ahead_mask, dec_padding_mask"
      ],
      "metadata": {
        "id": "dmnzYwOj7GDH"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hiperparâmetros\n"
      ],
      "metadata": {
        "id": "RFE3yvcK9M1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hiperparâmetros\n",
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1"
      ],
      "metadata": {
        "id": "Z464Ybto9bZP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Otimizador"
      ],
      "metadata": {
        "id": "n7l_0epu9jVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    step = tf.cast(step, tf.float32)\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
      ],
      "metadata": {
        "id": "vFtjEhWr9lQf"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Função de perda e métrica de Acurácia (mascarados)"
      ],
      "metadata": {
        "id": "rWdNHRIB-O_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "  accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  accuracies = tf.math.logical_and(mask, accuracies)\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "trains_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "trains_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
      ],
      "metadata": {
        "id": "WnEh6j1a-T7C"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinamento"
      ],
      "metadata": {
        "id": "OLt1_r4H-_nz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer(num_layers, d_model, num_heads, dff, tokenizers.pt.get_vocab_size().numpy(), tokenizers.en.get_vocab_size().numpy(), pe_input=1000, pe_target=1000, rate=dropout_rate)\n"
      ],
      "metadata": {
        "id": "v9P79S6d_A7m"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checkpoint"
      ],
      "metadata": {
        "id": "Qrfi8zzf_Pwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checkpoint\n",
        "checkpoint_path = \"./checkpoints/train\"\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print ('Latest checkpoint restored!!')"
      ],
      "metadata": {
        "id": "tMWLrR1G_RWE"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processo de treinamento"
      ],
      "metadata": {
        "id": "ezHfQD0g_vHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "]\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer([inp, tar_inp], training = True)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "  trains_loss(loss)\n",
        "  trains_accuracy(accuracy_function(tar_real, predictions))\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  trains_loss.reset_states()\n",
        "  trains_accuracy.reset_states()\n",
        "# imp -> portuguese, tar _> english\n",
        "  for (batch, (inp, tar)) in enumerate(train_batches):\n",
        "    train_step(inp, tar)\n",
        "    if batch % 50 == 0:\n",
        "      print (f'Epoch {epoch + 1} Batch {batch} Loss {trains_loss.result():.4f} Accuracy {trains_accuracy.result():.4f}')\n",
        "\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print (f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
        "\n",
        "  print (f'Epoch {epoch + 1} Loss {trains_loss.result():.4f} Accuracy {trains_accuracy.result():.4f}')\n",
        "\n",
        "  print (f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b6gaRv5_w1T",
        "outputId": "634fa946-5591-47e6-b707-afde46aaed0d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 8.8689 Accuracy 0.0000\n",
            "Epoch 1 Batch 50 Loss 8.8056 Accuracy 0.0022\n",
            "Epoch 1 Batch 100 Loss 8.6968 Accuracy 0.0228\n",
            "Epoch 1 Batch 150 Loss 8.5776 Accuracy 0.0311\n",
            "Epoch 1 Batch 200 Loss 8.4326 Accuracy 0.0351\n",
            "Epoch 1 Batch 250 Loss 8.2618 Accuracy 0.0376\n",
            "Epoch 1 Batch 300 Loss 8.0719 Accuracy 0.0418\n",
            "Epoch 1 Batch 350 Loss 7.8759 Accuracy 0.0485\n",
            "Epoch 1 Batch 400 Loss 7.6899 Accuracy 0.0551\n",
            "Epoch 1 Batch 450 Loss 7.5215 Accuracy 0.0633\n",
            "Epoch 1 Batch 500 Loss 7.3745 Accuracy 0.0722\n",
            "Epoch 1 Batch 550 Loss 7.2388 Accuracy 0.0805\n",
            "Epoch 1 Batch 600 Loss 7.1107 Accuracy 0.0891\n",
            "Epoch 1 Batch 650 Loss 6.9914 Accuracy 0.0968\n",
            "Epoch 1 Batch 700 Loss 6.8823 Accuracy 0.1039\n",
            "Epoch 1 Batch 750 Loss 6.7842 Accuracy 0.1102\n",
            "Epoch 1 Batch 800 Loss 6.6901 Accuracy 0.1165\n",
            "Epoch 1 Loss 6.6739 Accuracy 0.1175\n",
            "Time taken for 1 epoch: 179.96 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 5.2802 Accuracy 0.2127\n",
            "Epoch 2 Batch 50 Loss 5.2332 Accuracy 0.2126\n",
            "Epoch 2 Batch 100 Loss 5.1927 Accuracy 0.2154\n",
            "Epoch 2 Batch 150 Loss 5.1553 Accuracy 0.2195\n",
            "Epoch 2 Batch 200 Loss 5.1307 Accuracy 0.2224\n",
            "Epoch 2 Batch 250 Loss 5.1112 Accuracy 0.2245\n",
            "Epoch 2 Batch 300 Loss 5.0869 Accuracy 0.2270\n",
            "Epoch 2 Batch 350 Loss 5.0614 Accuracy 0.2297\n",
            "Epoch 2 Batch 400 Loss 5.0419 Accuracy 0.2315\n",
            "Epoch 2 Batch 450 Loss 5.0198 Accuracy 0.2339\n",
            "Epoch 2 Batch 500 Loss 5.0024 Accuracy 0.2356\n",
            "Epoch 2 Batch 550 Loss 4.9832 Accuracy 0.2375\n",
            "Epoch 2 Batch 600 Loss 4.9675 Accuracy 0.2389\n",
            "Epoch 2 Batch 650 Loss 4.9503 Accuracy 0.2406\n",
            "Epoch 2 Batch 700 Loss 4.9326 Accuracy 0.2422\n",
            "Epoch 2 Batch 750 Loss 4.9151 Accuracy 0.2437\n",
            "Epoch 2 Batch 800 Loss 4.8988 Accuracy 0.2452\n",
            "Epoch 2 Loss 4.8957 Accuracy 0.2454\n",
            "Time taken for 1 epoch: 103.98 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 4.5213 Accuracy 0.2856\n",
            "Epoch 3 Batch 50 Loss 4.6085 Accuracy 0.2700\n",
            "Epoch 3 Batch 100 Loss 4.5775 Accuracy 0.2726\n",
            "Epoch 3 Batch 150 Loss 4.5710 Accuracy 0.2733\n",
            "Epoch 3 Batch 200 Loss 4.5671 Accuracy 0.2731\n",
            "Epoch 3 Batch 250 Loss 4.5557 Accuracy 0.2741\n",
            "Epoch 3 Batch 300 Loss 4.5452 Accuracy 0.2751\n",
            "Epoch 3 Batch 350 Loss 4.5351 Accuracy 0.2761\n",
            "Epoch 3 Batch 400 Loss 4.5249 Accuracy 0.2771\n",
            "Epoch 3 Batch 450 Loss 4.5175 Accuracy 0.2777\n",
            "Epoch 3 Batch 500 Loss 4.5061 Accuracy 0.2787\n",
            "Epoch 3 Batch 550 Loss 4.4935 Accuracy 0.2801\n",
            "Epoch 3 Batch 600 Loss 4.4826 Accuracy 0.2812\n",
            "Epoch 3 Batch 650 Loss 4.4677 Accuracy 0.2828\n",
            "Epoch 3 Batch 700 Loss 4.4560 Accuracy 0.2842\n",
            "Epoch 3 Batch 750 Loss 4.4422 Accuracy 0.2858\n",
            "Epoch 3 Batch 800 Loss 4.4278 Accuracy 0.2875\n",
            "Epoch 3 Loss 4.4249 Accuracy 0.2878\n",
            "Time taken for 1 epoch: 100.09 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 3.9623 Accuracy 0.3526\n",
            "Epoch 4 Batch 50 Loss 4.1241 Accuracy 0.3203\n",
            "Epoch 4 Batch 100 Loss 4.0984 Accuracy 0.3247\n",
            "Epoch 4 Batch 150 Loss 4.0818 Accuracy 0.3262\n",
            "Epoch 4 Batch 200 Loss 4.0794 Accuracy 0.3262\n",
            "Epoch 4 Batch 250 Loss 4.0656 Accuracy 0.3279\n",
            "Epoch 4 Batch 300 Loss 4.0534 Accuracy 0.3291\n",
            "Epoch 4 Batch 350 Loss 4.0432 Accuracy 0.3305\n",
            "Epoch 4 Batch 400 Loss 4.0268 Accuracy 0.3324\n",
            "Epoch 4 Batch 450 Loss 4.0110 Accuracy 0.3344\n",
            "Epoch 4 Batch 500 Loss 3.9918 Accuracy 0.3370\n",
            "Epoch 4 Batch 550 Loss 3.9758 Accuracy 0.3390\n",
            "Epoch 4 Batch 600 Loss 3.9616 Accuracy 0.3412\n",
            "Epoch 4 Batch 650 Loss 3.9454 Accuracy 0.3434\n",
            "Epoch 4 Batch 700 Loss 3.9288 Accuracy 0.3457\n",
            "Epoch 4 Batch 750 Loss 3.9175 Accuracy 0.3474\n",
            "Epoch 4 Batch 800 Loss 3.9031 Accuracy 0.3493\n",
            "Epoch 4 Loss 3.9002 Accuracy 0.3497\n",
            "Time taken for 1 epoch: 100.45 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 3.6068 Accuracy 0.3659\n",
            "Epoch 5 Batch 50 Loss 3.5722 Accuracy 0.3879\n",
            "Epoch 5 Batch 100 Loss 3.5645 Accuracy 0.3904\n",
            "Epoch 5 Batch 150 Loss 3.5433 Accuracy 0.3930\n",
            "Epoch 5 Batch 200 Loss 3.5395 Accuracy 0.3935\n",
            "Epoch 5 Batch 250 Loss 3.5300 Accuracy 0.3947\n",
            "Epoch 5 Batch 300 Loss 3.5226 Accuracy 0.3961\n",
            "Epoch 5 Batch 350 Loss 3.5113 Accuracy 0.3977\n",
            "Epoch 5 Batch 400 Loss 3.5008 Accuracy 0.3990\n",
            "Epoch 5 Batch 450 Loss 3.4909 Accuracy 0.4006\n",
            "Epoch 5 Batch 500 Loss 3.4804 Accuracy 0.4019\n",
            "Epoch 5 Batch 550 Loss 3.4703 Accuracy 0.4036\n",
            "Epoch 5 Batch 600 Loss 3.4601 Accuracy 0.4049\n",
            "Epoch 5 Batch 650 Loss 3.4539 Accuracy 0.4059\n",
            "Epoch 5 Batch 700 Loss 3.4451 Accuracy 0.4071\n",
            "Epoch 5 Batch 750 Loss 3.4361 Accuracy 0.4083\n",
            "Epoch 5 Batch 800 Loss 3.4291 Accuracy 0.4092\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1\n",
            "Epoch 5 Loss 3.4267 Accuracy 0.4096\n",
            "Time taken for 1 epoch: 100.21 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 3.1945 Accuracy 0.4362\n",
            "Epoch 6 Batch 50 Loss 3.1248 Accuracy 0.4447\n",
            "Epoch 6 Batch 100 Loss 3.1315 Accuracy 0.4432\n",
            "Epoch 6 Batch 150 Loss 3.1273 Accuracy 0.4435\n",
            "Epoch 6 Batch 200 Loss 3.1274 Accuracy 0.4433\n",
            "Epoch 6 Batch 250 Loss 3.1154 Accuracy 0.4450\n",
            "Epoch 6 Batch 300 Loss 3.1074 Accuracy 0.4462\n",
            "Epoch 6 Batch 350 Loss 3.0993 Accuracy 0.4474\n",
            "Epoch 6 Batch 400 Loss 3.0866 Accuracy 0.4495\n",
            "Epoch 6 Batch 450 Loss 3.0795 Accuracy 0.4502\n",
            "Epoch 6 Batch 500 Loss 3.0719 Accuracy 0.4514\n",
            "Epoch 6 Batch 550 Loss 3.0630 Accuracy 0.4527\n",
            "Epoch 6 Batch 600 Loss 3.0579 Accuracy 0.4536\n",
            "Epoch 6 Batch 650 Loss 3.0515 Accuracy 0.4546\n",
            "Epoch 6 Batch 700 Loss 3.0442 Accuracy 0.4555\n",
            "Epoch 6 Batch 750 Loss 3.0356 Accuracy 0.4568\n",
            "Epoch 6 Batch 800 Loss 3.0288 Accuracy 0.4578\n",
            "Epoch 6 Loss 3.0285 Accuracy 0.4578\n",
            "Time taken for 1 epoch: 100.68 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 2.7065 Accuracy 0.4959\n",
            "Epoch 7 Batch 50 Loss 2.7412 Accuracy 0.4930\n",
            "Epoch 7 Batch 100 Loss 2.7235 Accuracy 0.4954\n",
            "Epoch 7 Batch 150 Loss 2.7374 Accuracy 0.4934\n",
            "Epoch 7 Batch 200 Loss 2.7361 Accuracy 0.4935\n",
            "Epoch 7 Batch 250 Loss 2.7355 Accuracy 0.4938\n",
            "Epoch 7 Batch 300 Loss 2.7310 Accuracy 0.4948\n",
            "Epoch 7 Batch 350 Loss 2.7304 Accuracy 0.4953\n",
            "Epoch 7 Batch 400 Loss 2.7251 Accuracy 0.4958\n",
            "Epoch 7 Batch 450 Loss 2.7153 Accuracy 0.4974\n",
            "Epoch 7 Batch 500 Loss 2.7092 Accuracy 0.4982\n",
            "Epoch 7 Batch 550 Loss 2.7070 Accuracy 0.4986\n",
            "Epoch 7 Batch 600 Loss 2.7028 Accuracy 0.4993\n",
            "Epoch 7 Batch 650 Loss 2.6982 Accuracy 0.5001\n",
            "Epoch 7 Batch 700 Loss 2.6953 Accuracy 0.5007\n",
            "Epoch 7 Batch 750 Loss 2.6918 Accuracy 0.5012\n",
            "Epoch 7 Batch 800 Loss 2.6882 Accuracy 0.5016\n",
            "Epoch 7 Loss 2.6869 Accuracy 0.5019\n",
            "Time taken for 1 epoch: 99.98 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 2.3165 Accuracy 0.5640\n",
            "Epoch 8 Batch 50 Loss 2.4882 Accuracy 0.5253\n",
            "Epoch 8 Batch 100 Loss 2.4818 Accuracy 0.5275\n",
            "Epoch 8 Batch 150 Loss 2.4782 Accuracy 0.5281\n",
            "Epoch 8 Batch 200 Loss 2.4766 Accuracy 0.5284\n",
            "Epoch 8 Batch 250 Loss 2.4747 Accuracy 0.5289\n",
            "Epoch 8 Batch 300 Loss 2.4743 Accuracy 0.5289\n",
            "Epoch 8 Batch 350 Loss 2.4698 Accuracy 0.5295\n",
            "Epoch 8 Batch 400 Loss 2.4714 Accuracy 0.5291\n",
            "Epoch 8 Batch 450 Loss 2.4672 Accuracy 0.5296\n",
            "Epoch 8 Batch 500 Loss 2.4607 Accuracy 0.5308\n",
            "Epoch 8 Batch 550 Loss 2.4546 Accuracy 0.5318\n",
            "Epoch 8 Batch 600 Loss 2.4523 Accuracy 0.5321\n",
            "Epoch 8 Batch 650 Loss 2.4521 Accuracy 0.5323\n",
            "Epoch 8 Batch 700 Loss 2.4477 Accuracy 0.5332\n",
            "Epoch 8 Batch 750 Loss 2.4437 Accuracy 0.5339\n",
            "Epoch 8 Batch 800 Loss 2.4443 Accuracy 0.5341\n",
            "Epoch 8 Loss 2.4442 Accuracy 0.5340\n",
            "Time taken for 1 epoch: 98.93 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 2.1301 Accuracy 0.5972\n",
            "Epoch 9 Batch 50 Loss 2.2564 Accuracy 0.5571\n",
            "Epoch 9 Batch 100 Loss 2.2558 Accuracy 0.5581\n",
            "Epoch 9 Batch 150 Loss 2.2588 Accuracy 0.5576\n",
            "Epoch 9 Batch 200 Loss 2.2639 Accuracy 0.5568\n",
            "Epoch 9 Batch 250 Loss 2.2691 Accuracy 0.5563\n",
            "Epoch 9 Batch 300 Loss 2.2662 Accuracy 0.5577\n",
            "Epoch 9 Batch 350 Loss 2.2614 Accuracy 0.5588\n",
            "Epoch 9 Batch 400 Loss 2.2640 Accuracy 0.5582\n",
            "Epoch 9 Batch 450 Loss 2.2626 Accuracy 0.5584\n",
            "Epoch 9 Batch 500 Loss 2.2625 Accuracy 0.5586\n",
            "Epoch 9 Batch 550 Loss 2.2594 Accuracy 0.5593\n",
            "Epoch 9 Batch 600 Loss 2.2618 Accuracy 0.5590\n",
            "Epoch 9 Batch 650 Loss 2.2603 Accuracy 0.5593\n",
            "Epoch 9 Batch 700 Loss 2.2610 Accuracy 0.5594\n",
            "Epoch 9 Batch 750 Loss 2.2604 Accuracy 0.5596\n",
            "Epoch 9 Batch 800 Loss 2.2583 Accuracy 0.5600\n",
            "Epoch 9 Loss 2.2578 Accuracy 0.5601\n",
            "Time taken for 1 epoch: 99.11 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 2.3554 Accuracy 0.5380\n",
            "Epoch 10 Batch 50 Loss 2.0746 Accuracy 0.5844\n",
            "Epoch 10 Batch 100 Loss 2.0821 Accuracy 0.5838\n",
            "Epoch 10 Batch 150 Loss 2.1026 Accuracy 0.5811\n",
            "Epoch 10 Batch 200 Loss 2.1071 Accuracy 0.5808\n",
            "Epoch 10 Batch 250 Loss 2.1133 Accuracy 0.5804\n",
            "Epoch 10 Batch 300 Loss 2.1108 Accuracy 0.5805\n",
            "Epoch 10 Batch 350 Loss 2.1134 Accuracy 0.5801\n",
            "Epoch 10 Batch 400 Loss 2.1122 Accuracy 0.5805\n",
            "Epoch 10 Batch 450 Loss 2.1111 Accuracy 0.5807\n",
            "Epoch 10 Batch 500 Loss 2.1118 Accuracy 0.5807\n",
            "Epoch 10 Batch 550 Loss 2.1095 Accuracy 0.5811\n",
            "Epoch 10 Batch 600 Loss 2.1115 Accuracy 0.5808\n",
            "Epoch 10 Batch 650 Loss 2.1112 Accuracy 0.5809\n",
            "Epoch 10 Batch 700 Loss 2.1113 Accuracy 0.5810\n",
            "Epoch 10 Batch 750 Loss 2.1114 Accuracy 0.5810\n",
            "Epoch 10 Batch 800 Loss 2.1110 Accuracy 0.5810\n",
            "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-2\n",
            "Epoch 10 Loss 2.1108 Accuracy 0.5810\n",
            "Time taken for 1 epoch: 100.12 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 2.0577 Accuracy 0.5976\n",
            "Epoch 11 Batch 50 Loss 2.0310 Accuracy 0.5870\n",
            "Epoch 11 Batch 100 Loss 1.9829 Accuracy 0.5963\n",
            "Epoch 11 Batch 150 Loss 1.9805 Accuracy 0.5970\n",
            "Epoch 11 Batch 200 Loss 1.9811 Accuracy 0.5982\n",
            "Epoch 11 Batch 250 Loss 1.9828 Accuracy 0.5982\n",
            "Epoch 11 Batch 300 Loss 1.9806 Accuracy 0.5988\n",
            "Epoch 11 Batch 350 Loss 1.9819 Accuracy 0.5986\n",
            "Epoch 11 Batch 400 Loss 1.9823 Accuracy 0.5988\n",
            "Epoch 11 Batch 450 Loss 1.9824 Accuracy 0.5990\n",
            "Epoch 11 Batch 500 Loss 1.9858 Accuracy 0.5983\n",
            "Epoch 11 Batch 550 Loss 1.9859 Accuracy 0.5985\n",
            "Epoch 11 Batch 600 Loss 1.9861 Accuracy 0.5987\n",
            "Epoch 11 Batch 650 Loss 1.9893 Accuracy 0.5982\n",
            "Epoch 11 Batch 700 Loss 1.9917 Accuracy 0.5976\n",
            "Epoch 11 Batch 750 Loss 1.9922 Accuracy 0.5978\n",
            "Epoch 11 Batch 800 Loss 1.9944 Accuracy 0.5975\n",
            "Epoch 11 Loss 1.9946 Accuracy 0.5975\n",
            "Time taken for 1 epoch: 99.20 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 1.9536 Accuracy 0.5920\n",
            "Epoch 12 Batch 50 Loss 1.8637 Accuracy 0.6144\n",
            "Epoch 12 Batch 100 Loss 1.8521 Accuracy 0.6168\n",
            "Epoch 12 Batch 150 Loss 1.8671 Accuracy 0.6144\n",
            "Epoch 12 Batch 200 Loss 1.8820 Accuracy 0.6122\n",
            "Epoch 12 Batch 250 Loss 1.8872 Accuracy 0.6114\n",
            "Epoch 12 Batch 300 Loss 1.8865 Accuracy 0.6124\n",
            "Epoch 12 Batch 350 Loss 1.8857 Accuracy 0.6128\n",
            "Epoch 12 Batch 400 Loss 1.8861 Accuracy 0.6129\n",
            "Epoch 12 Batch 450 Loss 1.8863 Accuracy 0.6129\n",
            "Epoch 12 Batch 500 Loss 1.8856 Accuracy 0.6132\n",
            "Epoch 12 Batch 550 Loss 1.8878 Accuracy 0.6128\n",
            "Epoch 12 Batch 600 Loss 1.8899 Accuracy 0.6124\n",
            "Epoch 12 Batch 650 Loss 1.8919 Accuracy 0.6121\n",
            "Epoch 12 Batch 700 Loss 1.8914 Accuracy 0.6125\n",
            "Epoch 12 Batch 750 Loss 1.8954 Accuracy 0.6119\n",
            "Epoch 12 Batch 800 Loss 1.8957 Accuracy 0.6122\n",
            "Epoch 12 Loss 1.8961 Accuracy 0.6121\n",
            "Time taken for 1 epoch: 98.72 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 1.6777 Accuracy 0.6518\n",
            "Epoch 13 Batch 50 Loss 1.7966 Accuracy 0.6245\n",
            "Epoch 13 Batch 100 Loss 1.7920 Accuracy 0.6274\n",
            "Epoch 13 Batch 150 Loss 1.7902 Accuracy 0.6277\n",
            "Epoch 13 Batch 200 Loss 1.7945 Accuracy 0.6268\n",
            "Epoch 13 Batch 250 Loss 1.7984 Accuracy 0.6259\n",
            "Epoch 13 Batch 300 Loss 1.7981 Accuracy 0.6261\n",
            "Epoch 13 Batch 350 Loss 1.7967 Accuracy 0.6267\n",
            "Epoch 13 Batch 400 Loss 1.7998 Accuracy 0.6262\n",
            "Epoch 13 Batch 450 Loss 1.8035 Accuracy 0.6255\n",
            "Epoch 13 Batch 500 Loss 1.8061 Accuracy 0.6252\n",
            "Epoch 13 Batch 550 Loss 1.8078 Accuracy 0.6248\n",
            "Epoch 13 Batch 600 Loss 1.8123 Accuracy 0.6242\n",
            "Epoch 13 Batch 650 Loss 1.8121 Accuracy 0.6242\n",
            "Epoch 13 Batch 700 Loss 1.8128 Accuracy 0.6243\n",
            "Epoch 13 Batch 750 Loss 1.8139 Accuracy 0.6243\n",
            "Epoch 13 Batch 800 Loss 1.8158 Accuracy 0.6241\n",
            "Epoch 13 Loss 1.8158 Accuracy 0.6241\n",
            "Time taken for 1 epoch: 99.02 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 1.5641 Accuracy 0.6785\n",
            "Epoch 14 Batch 50 Loss 1.7069 Accuracy 0.6398\n",
            "Epoch 14 Batch 100 Loss 1.7015 Accuracy 0.6400\n",
            "Epoch 14 Batch 150 Loss 1.7074 Accuracy 0.6387\n",
            "Epoch 14 Batch 200 Loss 1.7090 Accuracy 0.6389\n",
            "Epoch 14 Batch 250 Loss 1.7105 Accuracy 0.6391\n",
            "Epoch 14 Batch 300 Loss 1.7167 Accuracy 0.6378\n",
            "Epoch 14 Batch 350 Loss 1.7197 Accuracy 0.6377\n",
            "Epoch 14 Batch 400 Loss 1.7200 Accuracy 0.6377\n",
            "Epoch 14 Batch 450 Loss 1.7236 Accuracy 0.6372\n",
            "Epoch 14 Batch 500 Loss 1.7257 Accuracy 0.6371\n",
            "Epoch 14 Batch 550 Loss 1.7285 Accuracy 0.6365\n",
            "Epoch 14 Batch 600 Loss 1.7316 Accuracy 0.6362\n",
            "Epoch 14 Batch 650 Loss 1.7343 Accuracy 0.6357\n",
            "Epoch 14 Batch 700 Loss 1.7394 Accuracy 0.6351\n",
            "Epoch 14 Batch 750 Loss 1.7421 Accuracy 0.6348\n",
            "Epoch 14 Batch 800 Loss 1.7422 Accuracy 0.6350\n",
            "Epoch 14 Loss 1.7423 Accuracy 0.6349\n",
            "Time taken for 1 epoch: 99.26 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 1.6055 Accuracy 0.6638\n",
            "Epoch 15 Batch 50 Loss 1.6308 Accuracy 0.6514\n",
            "Epoch 15 Batch 100 Loss 1.6408 Accuracy 0.6505\n",
            "Epoch 15 Batch 150 Loss 1.6431 Accuracy 0.6502\n",
            "Epoch 15 Batch 200 Loss 1.6532 Accuracy 0.6484\n",
            "Epoch 15 Batch 250 Loss 1.6578 Accuracy 0.6476\n",
            "Epoch 15 Batch 300 Loss 1.6590 Accuracy 0.6474\n",
            "Epoch 15 Batch 350 Loss 1.6640 Accuracy 0.6465\n",
            "Epoch 15 Batch 400 Loss 1.6661 Accuracy 0.6464\n",
            "Epoch 15 Batch 450 Loss 1.6660 Accuracy 0.6466\n",
            "Epoch 15 Batch 500 Loss 1.6686 Accuracy 0.6463\n",
            "Epoch 15 Batch 550 Loss 1.6695 Accuracy 0.6460\n",
            "Epoch 15 Batch 600 Loss 1.6720 Accuracy 0.6457\n",
            "Epoch 15 Batch 650 Loss 1.6735 Accuracy 0.6457\n",
            "Epoch 15 Batch 700 Loss 1.6773 Accuracy 0.6450\n",
            "Epoch 15 Batch 750 Loss 1.6784 Accuracy 0.6449\n",
            "Epoch 15 Batch 800 Loss 1.6814 Accuracy 0.6445\n",
            "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-3\n",
            "Epoch 15 Loss 1.6809 Accuracy 0.6446\n",
            "Time taken for 1 epoch: 99.82 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 1.6294 Accuracy 0.6432\n",
            "Epoch 16 Batch 50 Loss 1.5869 Accuracy 0.6576\n",
            "Epoch 16 Batch 100 Loss 1.5841 Accuracy 0.6582\n",
            "Epoch 16 Batch 150 Loss 1.5933 Accuracy 0.6566\n",
            "Epoch 16 Batch 200 Loss 1.5997 Accuracy 0.6557\n",
            "Epoch 16 Batch 250 Loss 1.6013 Accuracy 0.6559\n",
            "Epoch 16 Batch 300 Loss 1.6027 Accuracy 0.6557\n",
            "Epoch 16 Batch 350 Loss 1.6050 Accuracy 0.6554\n",
            "Epoch 16 Batch 400 Loss 1.6079 Accuracy 0.6552\n",
            "Epoch 16 Batch 450 Loss 1.6068 Accuracy 0.6553\n",
            "Epoch 16 Batch 500 Loss 1.6098 Accuracy 0.6549\n",
            "Epoch 16 Batch 550 Loss 1.6119 Accuracy 0.6546\n",
            "Epoch 16 Batch 600 Loss 1.6139 Accuracy 0.6543\n",
            "Epoch 16 Batch 650 Loss 1.6173 Accuracy 0.6538\n",
            "Epoch 16 Batch 700 Loss 1.6207 Accuracy 0.6531\n",
            "Epoch 16 Batch 750 Loss 1.6232 Accuracy 0.6528\n",
            "Epoch 16 Batch 800 Loss 1.6272 Accuracy 0.6521\n",
            "Epoch 16 Loss 1.6266 Accuracy 0.6522\n",
            "Time taken for 1 epoch: 99.56 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 1.5408 Accuracy 0.6585\n",
            "Epoch 17 Batch 50 Loss 1.5185 Accuracy 0.6692\n",
            "Epoch 17 Batch 100 Loss 1.5246 Accuracy 0.6676\n",
            "Epoch 17 Batch 150 Loss 1.5314 Accuracy 0.6669\n",
            "Epoch 17 Batch 200 Loss 1.5367 Accuracy 0.6658\n",
            "Epoch 17 Batch 250 Loss 1.5472 Accuracy 0.6640\n",
            "Epoch 17 Batch 300 Loss 1.5503 Accuracy 0.6637\n",
            "Epoch 17 Batch 350 Loss 1.5521 Accuracy 0.6638\n",
            "Epoch 17 Batch 400 Loss 1.5527 Accuracy 0.6639\n",
            "Epoch 17 Batch 450 Loss 1.5558 Accuracy 0.6636\n",
            "Epoch 17 Batch 500 Loss 1.5568 Accuracy 0.6634\n",
            "Epoch 17 Batch 550 Loss 1.5597 Accuracy 0.6629\n",
            "Epoch 17 Batch 600 Loss 1.5652 Accuracy 0.6620\n",
            "Epoch 17 Batch 650 Loss 1.5662 Accuracy 0.6619\n",
            "Epoch 17 Batch 700 Loss 1.5692 Accuracy 0.6615\n",
            "Epoch 17 Batch 750 Loss 1.5724 Accuracy 0.6610\n",
            "Epoch 17 Batch 800 Loss 1.5758 Accuracy 0.6605\n",
            "Epoch 17 Loss 1.5768 Accuracy 0.6604\n",
            "Time taken for 1 epoch: 99.41 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 1.4509 Accuracy 0.6850\n",
            "Epoch 18 Batch 50 Loss 1.4988 Accuracy 0.6713\n",
            "Epoch 18 Batch 100 Loss 1.4984 Accuracy 0.6718\n",
            "Epoch 18 Batch 150 Loss 1.5020 Accuracy 0.6710\n",
            "Epoch 18 Batch 200 Loss 1.5036 Accuracy 0.6712\n",
            "Epoch 18 Batch 250 Loss 1.5063 Accuracy 0.6709\n",
            "Epoch 18 Batch 300 Loss 1.5081 Accuracy 0.6707\n",
            "Epoch 18 Batch 350 Loss 1.5109 Accuracy 0.6705\n",
            "Epoch 18 Batch 400 Loss 1.5145 Accuracy 0.6701\n",
            "Epoch 18 Batch 450 Loss 1.5144 Accuracy 0.6699\n",
            "Epoch 18 Batch 500 Loss 1.5150 Accuracy 0.6698\n",
            "Epoch 18 Batch 550 Loss 1.5186 Accuracy 0.6692\n",
            "Epoch 18 Batch 600 Loss 1.5201 Accuracy 0.6690\n",
            "Epoch 18 Batch 650 Loss 1.5218 Accuracy 0.6688\n",
            "Epoch 18 Batch 700 Loss 1.5244 Accuracy 0.6684\n",
            "Epoch 18 Batch 750 Loss 1.5263 Accuracy 0.6680\n",
            "Epoch 18 Batch 800 Loss 1.5306 Accuracy 0.6673\n",
            "Epoch 18 Loss 1.5305 Accuracy 0.6673\n",
            "Time taken for 1 epoch: 99.83 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 1.4441 Accuracy 0.6886\n",
            "Epoch 19 Batch 50 Loss 1.4328 Accuracy 0.6840\n",
            "Epoch 19 Batch 100 Loss 1.4440 Accuracy 0.6818\n",
            "Epoch 19 Batch 150 Loss 1.4480 Accuracy 0.6813\n",
            "Epoch 19 Batch 200 Loss 1.4559 Accuracy 0.6796\n",
            "Epoch 19 Batch 250 Loss 1.4576 Accuracy 0.6794\n",
            "Epoch 19 Batch 300 Loss 1.4596 Accuracy 0.6792\n",
            "Epoch 19 Batch 350 Loss 1.4635 Accuracy 0.6783\n",
            "Epoch 19 Batch 400 Loss 1.4667 Accuracy 0.6775\n",
            "Epoch 19 Batch 450 Loss 1.4682 Accuracy 0.6773\n",
            "Epoch 19 Batch 500 Loss 1.4717 Accuracy 0.6769\n",
            "Epoch 19 Batch 550 Loss 1.4759 Accuracy 0.6760\n",
            "Epoch 19 Batch 600 Loss 1.4795 Accuracy 0.6756\n",
            "Epoch 19 Batch 650 Loss 1.4832 Accuracy 0.6749\n",
            "Epoch 19 Batch 700 Loss 1.4866 Accuracy 0.6744\n",
            "Epoch 19 Batch 750 Loss 1.4889 Accuracy 0.6740\n",
            "Epoch 19 Batch 800 Loss 1.4928 Accuracy 0.6735\n",
            "Epoch 19 Loss 1.4937 Accuracy 0.6734\n",
            "Time taken for 1 epoch: 99.95 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 1.2724 Accuracy 0.7160\n",
            "Epoch 20 Batch 50 Loss 1.3922 Accuracy 0.6907\n",
            "Epoch 20 Batch 100 Loss 1.4050 Accuracy 0.6882\n",
            "Epoch 20 Batch 150 Loss 1.4173 Accuracy 0.6853\n",
            "Epoch 20 Batch 200 Loss 1.4218 Accuracy 0.6845\n",
            "Epoch 20 Batch 250 Loss 1.4279 Accuracy 0.6832\n",
            "Epoch 20 Batch 300 Loss 1.4296 Accuracy 0.6831\n",
            "Epoch 20 Batch 350 Loss 1.4299 Accuracy 0.6830\n",
            "Epoch 20 Batch 400 Loss 1.4326 Accuracy 0.6826\n",
            "Epoch 20 Batch 450 Loss 1.4372 Accuracy 0.6816\n",
            "Epoch 20 Batch 500 Loss 1.4388 Accuracy 0.6815\n",
            "Epoch 20 Batch 550 Loss 1.4405 Accuracy 0.6813\n",
            "Epoch 20 Batch 600 Loss 1.4407 Accuracy 0.6815\n",
            "Epoch 20 Batch 650 Loss 1.4442 Accuracy 0.6811\n",
            "Epoch 20 Batch 700 Loss 1.4485 Accuracy 0.6804\n",
            "Epoch 20 Batch 750 Loss 1.4514 Accuracy 0.6799\n",
            "Epoch 20 Batch 800 Loss 1.4556 Accuracy 0.6792\n",
            "Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-4\n",
            "Epoch 20 Loss 1.4564 Accuracy 0.6790\n",
            "Time taken for 1 epoch: 100.80 secs\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tradutor"
      ],
      "metadata": {
        "id": "i4ANzD3XBerd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Translator(tf.Module):\n",
        "  def __init__(self, tokenizers, transformer):\n",
        "    self.tokenizers = tokenizers\n",
        "    self.transformer = transformer\n",
        "\n",
        "  def __call__(self, sentence, max_length=20):\n",
        "    # input sentence in portuguese, hence adding the stard and end token\n",
        "    assert isinstance(sentence, tf.Tensor)\n",
        "    if len(sentence.shape) == 0:\n",
        "      sentence = sentence[tf.newaxis]\n",
        "    sentence = self.tokenizers.pt.tokenize(sentence).to_tensor()\n",
        "    encoder_input = sentence\n",
        "\n",
        "    # as the target is english, the first token to the transformer should be the english start token.\n",
        "    start_end = self.tokenizers.en.tokenize([''])[0]\n",
        "    start = start_end[0][tf.newaxis]\n",
        "    end = start_end[1][tf.newaxis]\n",
        "\n",
        "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
        "    output_array = output_array.write(0, start)\n",
        "\n",
        "    for i in tf.range(max_length):\n",
        "      output = tf.transpose(output_array.stack())\n",
        "      predictions, _ = self.transformer([encoder_input, output], training=False)\n",
        "      # select the last token from the seq_len dimension\n",
        "      predictions = predictions[:, -1:, :] # (batch_size, 1, vocab_size)\n",
        "      predicted_id = tf.argmax(predictions, axis=-1)\n",
        "      output_array = output_array.write(i+1, predicted_id[0])\n",
        "      if predicted_id == end:\n",
        "        break\n",
        "\n",
        "    output = tf.transpose(output_array.stack())\n",
        "    # output.shape (1, tokens)\n",
        "    text = tokenizers.en.detokenize(output)[0]\n",
        "    tokens = tokenizers.en.lookup(output)[0]\n",
        "\n",
        "    _, attention_weights = self.transformer([encoder_input, output[:,:-1]], training=False)\n",
        "\n",
        "    return text, tokens, attention_weights\n",
        ""
      ],
      "metadata": {
        "id": "dsiexyX0Bf3f"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Efetuando uma tradução"
      ],
      "metadata": {
        "id": "DpgvCGw0DCqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Efetuando uma tradução com o Transformer\n",
        "\n",
        "translator = Translator(tokenizers, transformer)\n",
        "sentence = \"Eu li sobre triceratops na enciclopédia.\"\n",
        "translated_text, translated_tokens, attention_weights = translator(tf.constant(sentence))\n",
        "\n",
        "print(translated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcIC_YEWDE9P",
        "outputId": "558ecccb-124b-4dce-be94-551afd549fa1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'i read about triumphs in the encyclopedia .', shape=(), dtype=string)\n"
          ]
        }
      ]
    }
  ]
}